{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation: The 1st Approach in Link Prediction\n",
    "In this Jupyter notebook, we embark on the journey of creating a dataset for link prediction in a graph and extracting various similarity metrics to better understand the relationships between nodes. This dataset will consist of potential edges, each labeled as follows:\n",
    "\n",
    "- Label `1` signifies that the edge represents an actual connection.\n",
    "- Label `0` indicates that the edge does not exist.\n",
    "\n",
    "We have access to information regarding the real edges, labeled as `1`. These real edges are divided into two categories: `Eprev` for training and `Enext` for testing. The challenge at hand is to construct non-existing edges (labeled as `0`) to create a well-balanced dataset. These non-existing edges belong to a set known as `Enon`, which is substantially larger in size compared to both `Eprev` and `Enext`.\n",
    "\n",
    "     The Approach\n",
    "The objective is to maintain this balance by implementing various strategies to sample non-existing edges, and subsequently comparing the effectiveness of these strategies. However, in this jyputer nootbook we will apply the first approach, which involves **`connecting less popular nodes to neighbors of more popular nodes`**. This strategy aims to create a dataset where non-existing edges exhibit similarities in terms of distance and Preferential Attachment to existing edges. Still, we will explore more alternative methods in the other notebooks to identify the most promising approach.\n",
    "\n",
    "Now, we embark on dataset creation, meticulously crafting the foundation for our link prediction. The subsequent notebook, titled 'Link_Prediction_of_1st_Approach', will contain a comprehensive evaluation of our dataset and the performance of this specific sampling strategy for the link prediction project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "from math import log\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63497050"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file = '../Dataset/stackoverflow_Graphs.csv'\n",
    "df = pd.read_csv(output_file)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Monotonic Increasing Timestamp Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Timestamp.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by= 'Timestamp',inplace= True)\n",
    "df.Timestamp.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Time Periods for Data Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the N parameter for monthly intervals\n",
    "N = int(2774 / 30 )\n",
    "# Calculate the set of time instances\n",
    "# -----------------------------------\n",
    "# Step 1: Calculate the time interval length\n",
    "DT =  df.Timestamp.max() - df.Timestamp.min()\n",
    "# Step 2: Calculate the length of each non-overlapping time period\n",
    "dt = DT / N\n",
    "# Step 3 : Determine the set of time instances {t0, ..., tN}\n",
    "time_instances = [df.Timestamp.min() + ( j * dt) for j in range(N+1)]\n",
    "\n",
    "# Calculate the set of non-overlapping time periods \n",
    "# -------------------------------------------------\n",
    "# The time_instances list will contain N+1 time instances which represent the start times of each non-overlapping time period {T1, ..., TN}\n",
    "# The end times of each time period will be the following time instance in the time_instances list\n",
    "# Determine the set of non-overlapping time periods {T1, ..., TN}\n",
    "time_periods = [(time_instances[i], time_instances[i + 1]) for i in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Creation and Subgraph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subgraph(subdf):\n",
    "    subgraph = nx.from_pandas_edgelist(subdf, 'Source_Node', 'Target_Node')\n",
    "    return subgraph\n",
    "\n",
    "def subgraphs_generator(df, time_periods):\n",
    "    for start, end in time_periods[:-1]:\n",
    "        subdf = df[(df.Timestamp >= start) & (df.Timestamp < end)]\n",
    "        subdf = subdf.drop_duplicates(subset=['Source_Node', 'Target_Node'])\n",
    "        subdf = subdf[subdf['Source_Node'] != subdf['Target_Node']]\n",
    "        subgraph = create_subgraph(subdf)\n",
    "        yield subgraph\n",
    "\n",
    "    start, end = time_periods[-1]\n",
    "    subdf = df[(df.Timestamp >= start) & (df.Timestamp <= end)]\n",
    "    subdf = subdf.drop_duplicates(subset=['Source_Node', 'Target_Node'])\n",
    "    subdf = subdf[subdf['Source_Node'] != subdf['Target_Node']]\n",
    "    subgraph = create_subgraph(subdf)\n",
    "    yield subgraph\n",
    "\n",
    "\n",
    "def ith_subgraph(df, time_periods, index):\n",
    "    if index < len(time_periods) - 1:\n",
    "        start, end = time_periods[index]\n",
    "        subdf = df[(df.Timestamp >= start) & (df.Timestamp < end)]\n",
    "        subdf = subdf.drop_duplicates(subset=['Source_Node', 'Target_Node'])\n",
    "        subdf = subdf[subdf['Source_Node'] != subdf['Target_Node']]\n",
    "        subgraph = create_subgraph(subdf)\n",
    "        return subgraph\n",
    "    else:\n",
    "        start, end = time_periods[-1]\n",
    "        subdf = df[(df.Timestamp >= start) & (df.Timestamp <= end)]\n",
    "        subdf = subdf.drop_duplicates(subset=['Source_Node', 'Target_Node'])\n",
    "        subdf = subdf[subdf['Source_Node'] != subdf['Target_Node']]\n",
    "        subgraph = create_subgraph(subdf)\n",
    "        return subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Common Nodes and Their Connections for Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the common set of nodes between two subgraphs\n",
    "def common_nodes(subgraph_current, subgraph_next):\n",
    "    return set(subgraph_current.nodes()).intersection(subgraph_next.nodes())\n",
    "\n",
    "# Function to find the restricted set of edges within the common set of nodes\n",
    "def restricted_edges(subgraph, common_nodes):\n",
    "    return [(u, v) for u, v in subgraph.edges() if (u in common_nodes and v in common_nodes)]\n",
    "\n",
    "# Lists to store the sets for each pair of successive time periods\n",
    "V_STAR = []\n",
    "E_STAR_PREV = []\n",
    "E_STAR_NEXT = []\n",
    "\n",
    "# Initialize the first subgraph as the previous / current\n",
    "graph_previous = ith_subgraph(df,time_periods,0)\n",
    "\n",
    "# Loop through each pair of successive time periods (Tj, Tj+1) where 1 ≤ j ≤ N - 1, \n",
    "# starting from the second time period because the first one is already initialized.\n",
    "for graph_next in subgraphs_generator(df, time_periods[1:]):\n",
    "\n",
    "    # Calculate the common set of nodes V∗[tj−1, tj+1]\n",
    "    v_star = common_nodes(graph_previous,graph_next)\n",
    "    V_STAR.append(len(v_star))\n",
    "\n",
    "    # Calculate the restricted sets of edges E∗[tj−1, tj] and E∗[tj, tj+1]\n",
    "    e_star_prev = restricted_edges(graph_previous, v_star)\n",
    "    e_star_next = restricted_edges(graph_next, v_star)\n",
    "    E_STAR_PREV.append(len(e_star_prev))\n",
    "    E_STAR_NEXT.append(len(e_star_next))\n",
    "\n",
    "    # Update the previous graph for the next iteration\n",
    "    graph_previous = graph_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is important to check if we have an empty set in V***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1713"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_STAR[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Functions To Calculating Similarity Metrics for Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the graph distance (SGD) between two nodes u and v in a subgraph\n",
    "def graph_distance(subgraph, u, v):\n",
    "    if nx.has_path(subgraph,u,v):\n",
    "        shortest_path = nx.shortest_path_length(subgraph, source=u, target=v)\n",
    "    else:\n",
    "        shortest_path = math.inf # If there is no path, return 0 to represent maximum distance (disconnected nodes)\n",
    "    return 1 / shortest_path\n",
    "\n",
    "# Calculate the common neighbors (SCN) between two nodes u and v in a subgraph\n",
    "def common_neighbors(subgraph, u, v):\n",
    "    neighbors_u = set(subgraph[u])\n",
    "    neighbors_v = set(subgraph[v])\n",
    "    return len(neighbors_u.intersection(neighbors_v))\n",
    "\n",
    "# Calculate Jaccard's coefficient (SJC) between two nodes u and v in a subgraph\n",
    "def jaccard_coefficient(subgraph, u, v):\n",
    "    neighbors_u = set(subgraph[u])\n",
    "    neighbors_v = set(subgraph[v])\n",
    "    union_size = len(neighbors_u.union(neighbors_v))\n",
    "    if union_size == 0:\n",
    "        return 0\n",
    "    return len(neighbors_u.intersection(neighbors_v)) / union_size\n",
    "\n",
    "# Calculate Adamic/Adar (SA) score between two nodes u and v in a subgraph\n",
    "def adamic_adar(subgraph, u, v):\n",
    "    common_neighbors = set(subgraph[u]).intersection(set(subgraph[v]))\n",
    "    if len(common_neighbors) == 0:\n",
    "        return 0\n",
    "    return sum(1 / log(len(subgraph[neighbor]), 2) if log(len(subgraph[neighbor]), 2) != 0 else 0 for neighbor in common_neighbors)\n",
    "\n",
    "# Calculate Preferential Attachment (SPA) between two nodes u and v in a subgraph\n",
    "def preferential_attachment(subgraph, u, v):\n",
    "    degree_u = subgraph.degree(u)\n",
    "    degree_v = subgraph.degree(v)\n",
    "    return degree_u * degree_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Metrics for Existing and Non-Existing Edges Based On the Approach\n",
    "\n",
    "In this section, we introduce our first approach for link prediction, which focuses on generating metrics for both existing and non-existing edges. The core idea is to calculate various similarity metrics that can be used to distinguish between actual connections (label `1`) and non-existing edges (label `0`). These metrics provide insights into the likelihood of a link between two nodes.\n",
    "\n",
    "    Metrics for Existing Edges (label 1)\n",
    "For existing edges (label `1`), we compute the following metrics to understand the similarities between connected nodes:\n",
    "- **Graph Distance (SGD):** Measures the distance between nodes in the subgraph.\n",
    "- **Common Neighbors (SCN):** Counts the number of shared neighbors between two nodes.\n",
    "- **Jaccard's Coefficient (SJC):** Measures the overlap between neighbor sets of two nodes.\n",
    "- **Adamic/Adar Score (SA):** Assigns a score based on shared neighbors, giving more weight to rarer neighbors.\n",
    "- **Preferential Attachment (SPA):** Computes the product of the number of neighbors of two nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Approach for Non-Existing Edges (label 0)\n",
    "Next, we delve into handling non-existing edges (label `0`). Our approach involves connecting a less popular node to the neighbors of a more popular node. By doing so, we aim to create non-existing edges that exhibit similarities in terms of distance and Preferential Attachment to existing edges. \n",
    "\n",
    "We implement this approach, iterating through the graph to identify potential non-existing edges and calculate their metrics. If the approach fails to find suitable neighbors for a less popular node within a certain limit (pos=5), a random node from the graph is chosen to create a non-existing edge. This method maintains the balance between existing and non-existing edges in our dataset, providing a foundation for further link prediction analysis.\n",
    "\n",
    "Please note that this is the first approach, and we will explore additional strategies in subsequent sections to identify the most promising method for link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "# Approach 1\n",
    "#----------------\n",
    "def get_metrics_gen(graph_star,e_star):\n",
    "    # Initialize a set to keep track of non-existing edges\n",
    "    e_non_ex = set()\n",
    "\n",
    "    # Iterate through each potential edge in the list of existing edges (e_star)\n",
    "    for (u,v) in list(e_star):\n",
    "\n",
    "        # Calculate and yield Metrics for Existing Edges (label 1)\n",
    "        dist = graph_distance(graph_star, u, v)\n",
    "        neighbours = common_neighbors(graph_star, u, v)\n",
    "        jaccard = jaccard_coefficient(graph_star, u, v)\n",
    "        adar = adamic_adar(graph_star, u, v)\n",
    "        pref = preferential_attachment(graph_star, u, v)\n",
    "        label = 1\n",
    "        yield (u,v),dist,neighbours,jaccard,adar,pref,label\n",
    "\n",
    "    # Implementing Approach for Non-Existing Edges (label 0)\n",
    "\n",
    "        # Get the degree of nodes u and v\n",
    "        u_degree = graph_star.degree(u)\n",
    "        v_degree = graph_star.degree(v)\n",
    "        \n",
    "        #Keep as source the less popular node\n",
    "        if u_degree <= v_degree:\n",
    "            source = u\n",
    "            target = v\n",
    "        else:\n",
    "            source = v\n",
    "            target = u\n",
    "\n",
    "        # Find the neighbors of the source and target nodes\n",
    "        source_neighbors = set(graph_star.neighbors(source))\n",
    "        target_neighbors = set(graph_star.neighbors(target))\n",
    "\n",
    "        # Find nodes not connected with the source\n",
    "        probable_future_connection_with_source = target_neighbors - target_neighbors.intersection(source_neighbors)\n",
    "        \n",
    "        # Identify nodes that are not connected with the source node\n",
    "        not_connected_with_source = [node for node in probable_future_connection_with_source if not graph_star.has_edge(node, source)]\n",
    "\n",
    "        pos = 0\n",
    "        # Attempt to find a suitable non-existing edge within a limit (pos=5)\n",
    "        while True:\n",
    "            if len(not_connected_with_source) > 0:\n",
    "                v_new = random.choice(not_connected_with_source)\n",
    "                if (source,v_new) not in e_star and (v_new,source) not in e_star and (source,v_new) not in e_non_ex and (v_new,source) not in e_non_ex and (source != v_new ):\n",
    "                    #print('not Random')\n",
    "                    break\n",
    "            if pos >= 5 or len(not_connected_with_source) == 0:\n",
    "                v_new = random.choice(list(graph_star.nodes))\n",
    "                if (source,v_new) not in e_star and (v_new,source) not in e_star and (source,v_new) not in e_non_ex and (v_new,source) not in e_non_ex and (source != v_new ):\n",
    "                    #print('random')\n",
    "                    break\n",
    "            pos += 1\n",
    "\n",
    "        # Add the non-existing edge to the set\n",
    "        e_non_ex.add((source,v_new))\n",
    "\n",
    "        # Calculate nad yield Metrics for Non-Existing Edges (label 0)\n",
    "        dist = graph_distance(graph_star, source, v_new)\n",
    "        neighbours = common_neighbors(graph_star, source, v_new)\n",
    "        jaccard = jaccard_coefficient(graph_star, source, v_new)\n",
    "        adar = adamic_adar(graph_star, source, v_new)\n",
    "        pref = preferential_attachment(graph_star, source, v_new)\n",
    "        label = 0\n",
    "        yield  (source,v_new) , dist, neighbours, jaccard, adar, pref, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Metrics for Training and Testing Datasets Based on Time Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics for training data\n",
    "possible_edges_train = []\n",
    "distance_train = []\n",
    "neighbors_train = []\n",
    "jaccard_train = []\n",
    "adar_train = []\n",
    "pref_train = []\n",
    "label_train = []\n",
    "\n",
    "# Initialize lists to store metrics for testing data\n",
    "possible_edges_test = []\n",
    "distance_test = []\n",
    "neighbors_test = []\n",
    "jaccard_test = []\n",
    "adar_test = []\n",
    "pref_test = []\n",
    "label_test = []\n",
    "c = 0\n",
    "\n",
    "# Initialize the previous graph as the first subgraph\n",
    "graph_previous = ith_subgraph(df,time_periods,0)\n",
    "\n",
    "# Iterate through each pair of successive time periods (Tj, Tj+1)\n",
    "for graph_next in subgraphs_generator(df, time_periods[1:]):\n",
    "        c +=1\n",
    "        #print(c) # Just count the iteration\n",
    "\n",
    "        # Calculate the common set of nodes (V*) between the previous and next subgraphs\n",
    "        v_star = common_nodes(graph_previous,graph_next)\n",
    "\n",
    "        # Calculate the restricted sets of edges E*[tj−1, tj] and E*[tj, tj+1]\n",
    "        e_star_prev = restricted_edges(graph_previous, v_star)\n",
    "        e_star_next = restricted_edges(graph_next, v_star)\n",
    "\n",
    "        # Create a graph (G_train) for the training data\n",
    "        G_train = nx.Graph()\n",
    "        G_train.add_nodes_from(v_star)\n",
    "        G_train.add_edges_from(e_star_prev)\n",
    "\n",
    "        # Create a graph (G_test) for the testing data\n",
    "        G_test = nx.Graph()\n",
    "        G_test.add_nodes_from(v_star)\n",
    "        G_test.add_edges_from(e_star_next)\n",
    "\n",
    "        # Calculate metrics for the training data\n",
    "        for possible_edge, sgd, scn, sjc, sa, spa,label  in get_metrics_gen(G_train,e_star_prev):\n",
    "                possible_edges_train.append(possible_edge)\n",
    "                distance_train.append(sgd)\n",
    "                neighbors_train.append(scn)\n",
    "                jaccard_train.append(sjc)\n",
    "                adar_train.append(sa)\n",
    "                pref_train.append(spa)\n",
    "                label_train.append(label)\n",
    "\n",
    "        # Calculate metrics for the testing data\n",
    "        for possible_edge, sgd, scn, sjc, sa, spa,label  in get_metrics_gen(G_test,e_star_next):\n",
    "                possible_edges_test.append(possible_edge)\n",
    "                distance_test.append(sgd)\n",
    "                neighbors_test.append(scn)\n",
    "                jaccard_test.append(sjc)\n",
    "                adar_test.append(sa)\n",
    "                pref_test.append(spa)\n",
    "                label_test.append(label)\n",
    "\n",
    "        # Update the previous graph for the next iteration\n",
    "        graph_previous = graph_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Training Metrics to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file path for traing data\n",
    "csv_file_path = \"../Dataset/monthly_1st_Approache_trainData.csv\"\n",
    "chunksize = 10000 # Define the chunk size for writing data\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['possible_edges', 'distance', 'neighbors', 'jaccard', 'adar', 'pref','label'])  # Write header\n",
    "    \n",
    "    chunk = [] # Initialize an empty chunk to store data temporarily\n",
    "    for row in zip(possible_edges_train, distance_train, neighbors_train, jaccard_train, adar_train, pref_train,label_train):\n",
    "        chunk.append(row) # Add the current row to the chunk\n",
    "        \n",
    "        if len(chunk) == chunksize:\n",
    "            writer.writerows(chunk) # Write the chunk to the CSV file\n",
    "            chunk = [] # Clear the chunk for the next set of data\n",
    "    \n",
    "    if chunk:\n",
    "        writer.writerows(chunk) # Write the remaining data in the chunk to the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Testing Metrics to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file path for testing data\n",
    "csv_file_path = \"../Dataset/monthly_1st_Approache_testData.csv\"\n",
    "chunksize = 10000\n",
    "\n",
    "# Create a CSV file for testing data\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['possible_edges', 'distance', 'neighbors', 'jaccard', 'adar', 'pref','label'])  # Write header\n",
    "    \n",
    "    chunk = []\n",
    "    for row in zip(possible_edges_test, distance_test, neighbors_test, jaccard_test, adar_test, pref_test,label_test):\n",
    "        chunk.append(row)\n",
    "        \n",
    "        if len(chunk) == chunksize:\n",
    "            writer.writerows(chunk)\n",
    "            chunk = []\n",
    "    \n",
    "    if chunk:\n",
    "        writer.writerows(chunk) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
